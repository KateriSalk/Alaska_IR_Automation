---
title: "Data Processing User Guide"
author: "Tetra Tech"
date: last-modified
date-format: "MMMM DD, YYYY"
format: docx
editor: visual
---

## Introduction

The following user guide pertains to the [*data_processing.R*](https://github.com/KateriSalk/Alaska_IR_Automation/blob/main/Code/5_Data_Processing/data_processing.R) script developed by Tetra Tech for the Alaska Department of Environmental Conservation (AK DEC). This guide assumes that the user has relatively recent versions of [R](https://www.r-project.org/) and [RStudio](https://posit.co/download/rstudio-desktop/) installed and that they are familiar with the R coding language. Please direct any questions regarding the usage of the *data_processing.R* script to Amber Bethe Crawford, AK DEC (amber.crawford\@alaska.gov).

### Purpose

The purpose of the *data_processing.R* script is to first conduct quality control and data wrangling on Water Quality Portal ([WQP](https://www.waterqualitydata.us/)) data downloaded using the [*data_pull.R*](https://github.com/KateriSalk/Alaska_IR_Automation/blob/main/Code/3_Data_Pull/data_pull.R) script. These steps heavily rely upon *TADA* functions as described below. WQ data are then matched to AK DEC's Assessment Units (AUs) using a crosswalk table. Monitoring locations with WQ data that are not in the crosswalk table are assigned to AUs using spatial joins to existing AU shapefiles and the joins can be evaluated using interactive mapping. Finally, data sufficiency is determined for each AU/pollutant combination using a data sufficiency lookup table for each designated use and pollutant.

## Required Packages

The required packages for the *data_processing.R* code are [*TADA*](https://github.com/USEPA/TADA), [*tidyverse*](https://www.tidyverse.org/)*, [leaflet](https://rstudio.github.io/leaflet/), [scales](https://scales.r-lib.org/),* and [*sf*](https://r-spatial.github.io/sf/)*.* Installation for the *TADA* and *tidyverse* packages can be found in the Data Pull Guide. Below is a code chunk that demonstrates how to install and load these packages.

```{r}
#| output: false
#| eval: false
# install.packages('leaflet')
library(leaflet)

# install.packages('scales')
library(scales)

# install.packages('sf')
library(sf)

library(TADA)
library(sf)
```

## Required Inputs

This code has specific exterior files that are required for it to run. The inputs are listed below. Note, only the outputs from *data_pull.R* are loaded at the beginning of the script, whereas all others are loaded when needed farther into the script.

| File Name                                              | Purpose                                      | Source                   |
|--------------------------------------------------------|----------------------------------------------|--------------------------|
| .CSV outputs (varies by site types from *data_pull.R*) | WQ data to be processed.                     | WQP (via *data_pull.R*). |
| WQ_Column_Manager.csv                                  | Quickly subset WQ dataset fields.            | Tetra Tech               |
| ML_AU_Crosswalk.csv                                    | Crosswalk Monitoring Locations with AUs/     | AK DEC                   |
| AK_DataSufficiency_Crosswalk_20231012.csv              | Determine data sufficiency per AU/pollutant. | AK DEC                   |
| Beaches.shp                                            | Beaches AU shapefile for mapping.            | AK DEC                   |
| Lakes.shp                                              | Lakes AU shapefile for mapping.              | AK DEC                   |
| MAUs_FINAL_2023.shp                                    | Marine AU shapefile for mapping.             | AK DEC                   |
| Rivers.shp                                             | Rivers AU shapefile for mapping.             | AK DEC                   |
| cb_2018_us_state_500k.shp                              | US States shapefile for mapping.             | Tetra Tech               |

: Exterior files required by data_processing.R to function properly, the purpose of each file within the code, and their source.

## Load Data

Data pulled from the WQP via the *data_pull.R* script are read in to the *data_processing.R* script for processing and quality control. The following lines read in the .CSVs by site type and explicitly excludes the composite dataset ('data_pull_all.csv') which allows for this code to be used for different combinations of files from *data_pull.R*. Data are then combined into a single dataset.

```{r}
#| eval: false
#Find all file names that end in .csv from the data_pull output folder
csv_names1 <- list.files('Data/data_pull', pattern = '.csv', full.names = T)
csv_names <- csv_names1[!str_detect(csv_names1, pattern = 'all')]
  
#Read in csvs and combine into one table
all_input_data <- tibble()
for(i in 1:length(csv_names)) {
  csv <- read_csv(csv_names[i])
  all_input_data <- all_input_data %>%
    rbind(csv)
  remove(csv)
}
```

## Section 1: Identifying TADA Flags

After the initial setup and data read, the data processing is broken up into numbered steps that are divided into five different sections. Steps #1 through #13 are in the 'Identify TADA Flags' section. Each step represents a different *TADA* function that scans the input dataset for quality control issues. Note, every *TADA* function has an argument that asks whether the data should be "cleaned" which automatically removes erroneous or suspect data. We did not use the "clean" arguments to ensure that the WQ data could be reviewed by an analyst at the end.

Each *TADA* function appends one or more columns to the input dataset and are either updated versions of existing columns or flags for potential erroneous data points (every new *TADA* field has a 'TADA.' prefix). Within *data_processing.R*, the comments under each step name list the column names that the *TADA* function adds to the data. Here is the example for Step #1:

```{r}
#| eval: false
#####1. Check Result Unit Validity#####
# This function adds the TADA.ResultUnit.Flag to the dataframe.
data_1 <- TADA_FlagResultUnit(all_input_data, clean = 'none')
```

The specifics for each *TADA* function can be found using the '?' function in R or by reading through the [Module 1 TADA vignette](https://github.com/USEPA/TADA/blob/develop/vignettes/TADAModule1.Rmd). The following functions are used to flag potential issues in the data (in order of appearance in *data_processing.R*):

1.  *TADA_FlagResultUnit*: Checks for errors in units

2.  *TADA_FlagFraction*: Checks for invalid characteristic-fraction combinations

3.  *TADA_FlagSpeciation*: Checks for invalid characteristic-method speciation

4.  *TADA_HarmonizeSynonyms*: Checks for duplicate naming and assigns a name based on a synonym reference table

5.  *TADA_FlagAboveThreshold/TADA_FlagBelowThreshold*: Checks for values above or below the threshold for that 'CharacteristicName'

6.  *TADA_FindContinuousData*: Checks metadata to flag any potential aggregated continuous data submitted to WQP

7.  *TADA_FlagMethod*: Checks for invalid characteristic-analytical method combinations

8.  *TADA_FindpotentialDuplicatesMultipleOrgs/TADA_FindPotentialDuplicatesSingleOrg*: Checks for duplicate samples within other organizations/within the same organization

9.  *TADA_FindQCActivities*: Identifies QC samples

10. *TADA_FlagCoordinates*: Checks for coordinates outside of the United States

11. *TADA_FlagMeasureQualifierCode*: Checks the 'MeasureQualifierCode' for any known suspect codes. Note, that any Measure Qualifier Codes not recognized by this function are flagged as 'uncategorized'. A sub-step in Step #11 manually replaces 'uncategorized' flags with appropriate flags based on a list of known Measure Qualifier Codes from AK DEC (see code chunk below). Any new flags encountered after the development of *data_processing.R* will require the analyst to manually append new flags into the code chunk.

    ```{r}
    #| eval: false
    # add any changes by making a new row below
    data_11b <- data_11a %>% 
      mutate(TADA.MeasureQualifierCode.Flag = 
            case_when((MeasureQualifierCode == "H;U") ~ "Suspect"
                , (MeasureQualifierCode == "RC;U") ~ "Non-Detect"
                , (MeasureQualifierCode == "H;RC;U") ~ "Suspect"
                , (MeasureQualifierCode == "J-R;TOC") ~ "Pass"
                , (MeasureQualifierCode == "TOC;U") ~ "Non-Detect"
                , (MeasureQualifierCode == "RC;SUS") ~ "Suspect"
                , (MeasureQualifierCode == "O;RC") ~ "Pass"
                , (MeasureQualifierCode == "H;RC") ~ "Suspect"
                , (MeasureQualifierCode == "B;J-R") ~ "Pass"
                , (MeasureQualifierCode == "H;J-R") ~ "Suspect"
                , (MeasureQualifierCode == "IQCOL;U") ~ "Pass"
                , (MeasureQualifierCode == "IQCOL;J-R") ~ "Pass"
                , (MeasureQualifierCode == "LL;RC") ~ "Pass"
                , (MeasureQualifierCode == "BQL;RC") ~ "Pass"
                , (MeasureQualifierCode == "B;D") ~ "Pass"
                , (MeasureQualifierCode == "SDROL;U") ~ "Suspect"
                , TRUE ~ TADA.MeasureQualifierCode.Flag))
    ```

12. *TADA_SimpleCensoredMethods*: Determines if a sample is non-detect and if it needs to reassigned - currently set to assign non-detects to 0.5 the detection limit

After the *TADA* functions are applied, Step #13 removes any columns that are entirely NA from the dataset.

```{r}
#| eval: false
#####13. Identify columns with all NA values#####
# Check whether you expect data in any of the columns listed below.
(cols_NA <- data_12 %>% 
   keep(~all(is.na(.x))) %>% 
   names)

# Eliminate any columns with all NA values
data_13 <- data_12 %>% 
  select(where(~sum(!is.na(.x)) > 0))

```

## Section 2: Evaluate and Trim Data

Section #2 uses the flags from Section #1 to remove erroneous data. This section is composed of Steps #14 through #18. The following is a breakdown of each step.

14. Create a data summary table with each row being a column from the Step #13 output in Section 1 (see screenshot of example output below). The fields within the summary table include the column name, class, number of unique values, and a list of those unique values (if the list is 10 or less in length). The data summary table provides the user with the means to quickly review field attributes which can be used to identify and remove suspect data.

    ![Example output from Step #14 'for loop' in data_processing.R](images/summary_data_table_head.PNG){width="815"}

15. The *WQ_Column_Manager.csv* is read in and the 'Keep_YN' column is used to filter out unnecessary columns. A quality control check ensures that the external manager file is up to date and prompts the user for an update if any new columns are detected in the WQ dataset.

16. The WQ dataset is trimmed by removing invalid data (mainly from *TADA* function fields) (see the code chunk below). The user can also review the data summary table generated in Step #14 to inform additional trimming steps. Note, that at the time of development, outputs from some *TADA* functions did not match the outputs described in the R documentation, therefore, the following crosswalk was assumed (see comments):

    ```{r}
    #| eval: false
    # Assume the following:
    # Not Reviewed <- "Not Reviewed" 
    # Valid <- c("Accepted", "Y")
    # Invalid <- c("Rejected", "Rejected ", "N")
    # NonStandardized <- c("NonStandardized",
    #                  "InvalidMediaUnit",
    #                  "InvalidChar",
    #                  "MethodNeeded")

    data_16 <- data_15 %>% 
      filter(TADA.ResultUnit.Flag != "Rejected") %>% # Step 1
      filter(TADA.SampleFraction.Flag != "Rejected") %>% # Step 2
      filter(TADA.MethodSpeciation.Flag != "Rejected") %>% # Step 3
      filter(TADA.AnalyticalMethod.Flag != "Rejected") %>% # Step 7
      filter(TADA.ActivityType.Flag == 'Non_QC') %>% # Step 9
      filter(TADA.MeasureQualifierCode.Flag != 'Suspect') %>% # Step 11
      filter(TADA.ActivityMediaName == 'WATER') # Remove non-water samples
    # censored data are retained in this dataset.
    ```

17. Create boxplots and log10 boxplots of each unique 'TADA.CharacteristicName' and site type through a for loop. Before the boxplots are created, NA values in 'TADA.ResultMeasureValue' are filtered out. Each site type is manually assigned a different color. These boxplots are exported to pdf for visual review. The following is an example boxplot for water temperature:

    ![](images/ak_boxplot_watertemp.PNG){width="491"}

18. Create an 'ultra trim' dataset to remove any unnecessary columns. At the time of development, only the columns listed in the code chunk below were kept. The user is welcome to retain any additional columns by appending their name to the code.

    ```{r}
    #| eval: false
    #####18. Ultra trim data#####
    data_18 <- data_16 %>% 
      select(OrganizationIdentifier
             ,ActivityStartDate
             ,MonitoringLocationIdentifier
             ,MonitoringLocationName
             ,MonitoringLocationTypeName
             ,TADA.CharacteristicName
             ,TADA.ResultMeasureValue
             ,TADA.ResultMeasure.MeasureUnitCode
             ,TADA.LatitudeMeasure
             ,TADA.LongitudeMeasure)
    ```

## Section 3: Match Data to AUs

This section matches the processed WQ data with the appropriate AK DEC Assessment Unit (AU). This section is made up of Steps #19 through #20, although Step #20 is broken into six sub-steps.

19. Read in *ML_AU_Crosswalk.csv* and join that with the output of Section 2 using the 'MonitoringLocationIdentifier' field. Monitoring locations from the WQ dataset that are already in the crosswalk table will be assigned an AU. This step also has optional code that uses *leaflet* to create an interactive map of the monitoring locations by waterbody type (displays in the RStudio 'Viewer' tab).
20. Assigning monitoring locations to their appropriate AU if it is not specified in the crosswalk from Step #19.
    a.  Read in the AU shapefiles and transform them to NAD83 Alaska Albers (EPSG: 3338).

    b.  Separate monitoring locations with missing AUs by waterbody type (beach, lake, marine, or river). Note, the waterbody types listed in 'MonitoringLocationTypeName' from WQP do not exactly match the waterbody types listed in *ML_AU_Crosswalk.csv*.

    c.  Select beach monitoring locations and assign them to the nearest beach AU using a spatial join. Distance between the monitoring location points and the AU polygons are calculated. The following code is for the beach waterbody type:

        ```{r}
        #| eval: false
        ######20b. Beaches #####
        miss_ML_beaches <- missing_ML %>% # filter appropriate sites
          filter(MonitoringLocationTypeName == "BEACH Program Site-Ocean")

        ### QC check
        num_sites <- nrow(miss_ML_beaches)

        if(num_sites == 0){
          print(paste("There are NO beach monitoring locations missing AU data."
                      ,"Skip to the next section."))
        } else {
          print(paste("There ARE", num_sites, "beach monitoring locations missing AU data."
                      ,"Continue to assign MLs to AUs using spatial joins."))
        }# end if/else statement

        ### convert to geospatial layer (sf object)
        beach_pts <- sf::st_as_sf(x = miss_ML_beaches, coords = c("TADA.LongitudeMeasure"
                                                                  ,"TADA.LatitudeMeasure")
                                  , crs = "+proj=longlat +datum=WGS84")%>% 
          sf::st_transform(st_crs(beach_shp))

        ### plot to see how they relate
        ggplot() +
          geom_sf(data = AK_shp)+
          geom_sf(data = beach_shp, color = "red") +
          geom_sf(data = beach_pts, color = "red") +
          theme_minimal()

        ### spatial join
        beach_SpatJoin <- sf::st_join(beach_pts, beach_shp, join = st_nearest_feature) %>% # join points and AUs
          select(MonitoringLocationIdentifier, MonitoringLocationName
                 , MonitoringLocationTypeName, AUID_ATTNS, Name_AU, HUC10) # trim unneccessary columns

        ### determine distance (m) between points and nearest feature
        near_feat <- sf::st_nearest_feature(beach_pts, beach_shp)
        dist_to_AU_m <- sf::st_distance(beach_pts, beach_shp[near_feat,], by_element = TRUE)

        ### join distance measurements to join results
        beach_SpatJoin2 <- cbind(beach_SpatJoin, dist_to_AU_m)

        ### results and export data
        miss_ML_beach_results <- beach_SpatJoin2 %>%
          sf::st_transform(4326) %>% 
          mutate(Longitude = unlist(map(geometry,1)),
                 Latitude = unlist(map(geometry,2))) %>% 
          sf::st_drop_geometry()
        ```

    d.  Repeat for each waterbody type.

    e.  Create an interactive map using *leaflet* with each of the AU waterbody types and the monitoring locations with missing AU assignments. Use this map and the outputs from the spatial joins to evaluate whether montioring locations were correctly assigned to AUs.

    f.  Update *ML_AU_Crosswalk.csv* with correct monitoring location and AU information based on the results from the previous sub-steps.

## Section 4: Organize Data by AU

This section aims to explore the data within each AU and provide summary statistics for each.

21. Create a boxplot of the frequency of a given number of monitoring locations within an AU. Create a summary table by AU and 'TADA.CharacteristicName' that provides the number of samples, minimum, median, maximum, 25th quantile, and 75th quantile values for each.

## Section 5: Data Sufficiency

The final section of the code compares the data sufficiency needs for AK DEC's Integrated Reporting process with the WQ data actually available. This section provides a final output that designates whether there is sufficient data for a given AU and pollutant combinations for assessment purposes.

22. Read in *AK_DataSufficiency_Crosswalk_20231012.csv*. For each AU in the WQ dataset, the data sufficiency table is filtered by waterbody type and the corresponding constituents. The number of samples and number of years of data for each constituent within a given AU is compared to the minimum requirements. If sufficient data is available, that constituent can be used at the assessment level for the given AU. Conversely, if insufficient, the data can only be used at the screening level.
